### 第4章 随机变量数字特征

#### 期望

> 名字来源于赌博，它其实就是以概率为权的加权平均



:goat: 对于连续型变量X, 如果$\int_{-\infty}^{+\infty }|a_i|\,p_i<\infty$,则 $E(x)=\int_{-\infty}^{+\infty} a_i p_i$ 为X的期望

:goat: 对于离散型变量X, 如果$\sum_{i=1}^{\infty }|a_i|\,p_i<\infty$,则 $E(x)=\sum_{i=1}^{\infty} a_i p_i$ 为X的期望



##### 常用离散的分布

若$X$~$P(\lambda)$,则$E(X)=\lambda$

若$X$~$B(n,p)$,则$E(X)=np$

若$X$~$G(p)$,则$E(X)=\frac{1}{p}$

若$X$~$N(\mu,\sigma^2)$,则$E(X)=\mu$

##### 性质

* $E(X_1+X_1)=E(X_1)+E(X_1)$



##### 条件数学期望

* 记$f_1(x)$为变量X在x点的概率密度函数$E(Y)=\int_{-\infty}^{\infty}E(Y|x)f_1(x)dx$

* $E(Y)=E[E(Y|X)]$

  

#### 中位数

相比于期望的优点：总是存在，而期望不一定存在

缺点：处理不方便，因为没有像期望一样有反映变量之前的关系的性质，如$E(X_1+X_1)=E(X_1)+E(X_1)$



#### 方差

设X是一个随机变量，若$E[X-E[X]^2]$存在，则称$D(X)=E[X-E[X]^2]$是X的方差，$\sqrt{D(x)}$ 是X的标准差，

* $D(X)=E(X^2)-E(X)^2$

* 设c是常数，则$D(cX)=c^2D(X)$

* 若X，Y独立，则$D(XY)=D(X)D(Y)+D(X)[E(Y)]^2+D(Y)[E(X)]^2$，

  

* 若X和Y独立，则$D(X\pm Y)=D(X)+D(Y)$,

  若X和Y不独立，则$D(X \pm Y)=D(X)+D(Y)\pm 2E\{[X-E(X)][Y-E(Y)]\}=D(X)+D(Y)\pm 2Cov(X,Y)$,

  

  #### 常用分布的期望和方差

<img src="http://mally.oss-cn-qingdao.aliyuncs.com/PicGo上传的图片/20200605/183811352.png" alt="mark" style="zoom:50%;" />

#### 协方差

cov(X,Y)=E{[X-E(X)] [Y-E(Y)]}=E(XY)-E(X)E(Y)

> $D(X \pm Y)\,=D(X)+D(Y)\pm 2E\{[X-E(X)][Y-E(Y)]\}\,=D(X)+D(Y)+2COV(X,Y)$,
>
> 则X和Y不独立，意味着 $E\{[X-E(X)][Y-E(Y)]\}\neq 0 $ 



* 协方差意义

  当Cov(X,Y)>0时，称X与Y正相关,即X和Y同时大于或同时小于它们的期望；

  当Cov(X,Y)<0时，称X与Y负相关；

  当Cov(X,Y)=0时，称X与Y不相关.

* 协方差性质

  cov（X,Y)=cov（Y,X)

  $cov（kX+a,lY）=kl\,cov（X,Y)$

  $cov（X_1+X_2,Y)=cov（X_1,Y)+cov（X_2,Y)$
  
  $cov(X,X)=D(X),\quad cov(X,c)=0$
  
  



#### 相关系数

$\rho_{XY}=\frac{cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$

1. 相关系数的性质

   * $\rho_{XY}=0$，则X与Y线性无关，X与Y不相关。

     （但不能推出X与Y独立，因为不知道其联合密度$f(x,y)$是否等于边缘密度$f(x)$和$f(y)$之积）

   * $|\rho_{XY}|=1,则X与Y完全线性相关$
   * $0<|\rho_{XY}|<1$,则X与Y之间有一定程度的线性关系而非严格的线性关系

2. 提出意义：

   假设由最小二乘法提出的a+bX已经最佳逼近，那么这种逼近的程度由$\rho_{XY}$ 决定

   （最佳逼近是使得$E[(Y-a-bX)^2]$ 达到最小，见陈希儒课本P127)

   

#### 矩

$E[(X-c)^k]$ 称为X关于c点的k阶矩

(1)c=0，这时$a_k=E(X^k)$称为X的k阶原点矩
(2)c=E(X),这时$\mu_k=E\{\,[X-E(X)]^k\,\}$称为X的k阶中心矩​



:baby_chick: 矩的实际意义

* $a_1$就是期望，

* $\mu_1=0,\mu_2就是方差$

* $\mu_3$衡量分布是否有偏差,比如正态分布中是左右对称，则$\mu_3$=0；如果分布左偏，则$\mu_3<0$

* $\mu_4$衡量分布（密度）在均值附近的陡峭程度。

  如果X取值在概率上集中在E(X)附近，则

分位数

某连续型随机变量X的分布函数为F(x)，密度函数为f(x),$F(\nu_p)=P(X<=\nu_p)=\int_{-\infty}^{\nu_p}f(x)dx=p$,

则称 $\nu_P=F^{-1}(p)$ 为X的p分位数，当$p=\frac{1}{2}$时，$\nu_{\frac{1}{2}}$ 为中位数。

## 第5章 大数定理及中心极限定理

### 大数定律

> 大数定律解释了为什么随着实验次数的增多，事件的频率逐渐稳定
>
> “定理”一词用于能用数学工具杨哥证明的东西，而“定律”则不是，比如电学的欧姆定律。

三者关系

<img src="http://mally.oss-cn-qingdao.aliyuncs.com/PicGo上传的图片/20200606/162920451.png" alt="mark" style="zoom:50%;" />

#### 切比雪夫不等式

设随机变量X的E(X)和D(X)都存在，则对于任意$\varepsilon>0$,有
$$
P(|X-E(X)|>=\varepsilon)\,<\,\frac{D(X)}{\varepsilon^2}
$$
**意义：**

​	$|X-E(X)|>=\varepsilon$ 理解为X关于期望发生的偏差，可见，方差决定了这个事件发生的概率上限。

​	在实际应用中，由X的观测数据估计得到X的期望和方差，然后用切比雪夫不等式估计X关于E(X)的偏离程度



#### 切比雪夫大数定律

:goat: $若X_1,X_2,....不相关，且D(X_i)有限，则对于任意\varepsilon>0,有$
$$
\lim_{n\rightarrow \infty}P(|\frac{1}{n}\sum_{i=1}^{n}X_i\,-\,\frac{1}{n}\sum_{i=1}^nE(X_i)|<\varepsilon)=1
$$
也可以表示成	$\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i  \rightarrow \frac{1}{n}\sum_{i=1}^n E(X_i) $







#### 辛钦大数定律

:goat: $若X_1,X_2,....相互独立同分布，且E(X_i)存在，则对于任意\varepsilon>0,有$
$$
\lim_{n\rightarrow \infty}P(|\frac{1}{n}\sum_{i=1}^{n}X_i-E(X_i)|<\varepsilon)=1
$$

也可以表示成	$\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i  \rightarrow E(X_i) $



1. **相互独立同分布**：是指随机变量序列相互独立且序列中随机变量的分布类型以及参数均相同

2. **直观意义：** $\overline{X_n}$依概率收敛于期望

   比如为了精确称量某个物体的质量，可在相同条件下重复称量n次，取平均值



#### 伯努利大数定律

:goat: $若X_1,X_2,....相互独立同分布，且X_i$~$B(1,p)，i=1,2,...,则对于任意\varepsilon>0,有$
$$
\lim_{n\rightarrow \infty}P(|\frac{1}{n}\sum_{i=1}^{n}X_i-p|<\varepsilon)=1
$$
也可以表示成	$\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i\rightarrow p$

1. **直观意义**：频率收敛于概率

   在大量相互独立重复实验中，用某个事件A发生的频率可以近似每次实验中事件A发生的概率

   

### 中心极限定理

> 习惯于把和的分布收敛于正态分布这一类定理都叫做中心极限定理.

#### 林德伯格-莱维定理.

:goat: $若X_1,X_2,....相互独立同分布，且E(X_i)=\mu,D(X_i)=\sigma^2(0<\sigma<\infty),则对于充分大的n有$
$$
\sum_{i=1}^n X_i  \rightarrow N(n\mu,n\sigma^2)
$$

$$
P(a<\sum_{i=1}^nX_i\leq b)\approx\Phi(\frac{b-n\mu}{\sqrt{n}\sigma})-\Phi(\frac{a-n\mu}{\sqrt{n}\sigma})
$$

因此也可以推的

* $$
  E(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n}*E(\sum_{i=1}^n X_i)=\mu,\quad\\
  D(\frac{1}{n}\sum_{i=1}^n X_i)=\frac{1}{n^2}*D(\sum_{i=1}^n X_i)=\frac{\sigma^2}{n}\\
\frac{1}{n}\sum_{i=1}^n X_i \Rightarrow N(\mu，\frac{\sigma^2}{n})
  $$



### 棣莫佛－拉普拉斯定理

:goat: 设$Y_n$ ~ B(n，p)，则对充分大的n，有$Y_n$ ~N(np,npq), （q=1-p），即
$$
P(a<\sum_{i=1}^nY_n\leq b)\approx\Phi(\frac{b-np}{\sqrt{npq}})-\Phi(\frac{a-np}{\sqrt{npq}})
$$
在实际中**, 0.1<p<0.9,** npq>**9时,** 用正态近似； 

当 p**≤0.1(或p≥0.9)** 且 n≥10时，用泊松近似.

